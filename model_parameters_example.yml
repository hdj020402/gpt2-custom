### General ###
mode: training   # training / hpo / generation
jobtype: ...
seed: 42

### Dataset ###
train_file: null
val_file: null
test_file: null
target: 'moltxt'
corpus_file: null
custom_tokens_file: null
vocab_size: 1000
tk_num_proc: 4
per_device_train_batch_size: 32
dataloader_num_workers: 4

### Model ###
n_ctx: 1024
n_embd: 768
n_head: 12
n_layer: 12
n_positions: 1024

### Training ###
gradient_accumulation_steps: 1
num_train_epochs: 500
learning_rate: 0.0001
weight_decay: 0.01
max_grad_norm: 1.0
warmup_steps: 500
eval_steps: 1000
model_save_steps: 1000
save_total_limit: 3
logging_steps: 1000
early_stopping:
  patience: 10
  threshold: 0

### Generation ###
pretrained_model: null
start_token: '<Energy:>'
stop_token: '</ei1a>'
temperature: 0.0
max_tokens: 100


